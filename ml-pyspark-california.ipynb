{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML with PySpark: California Housing\n",
    "In this notebook, I have done big data processing, analysis and ML with PySpark. Firstly, I have explored and preprocessed the dataset that I loaded in at the first step the help of DataFrames. About the dataset: It appeared in a 1997 paper titled Sparse Spatial Autoregressions, written by Pace, R. Kelley and Ronald Barry and published in the Statistics and Probability Letters journal. The researchers built this data set by using the 1990 California census data.\n",
    "\n",
    "The data contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). In this sample a block group on average includes 1425.5 individuals living in a geographically compact area. You’ll gather this information from this web page or by reading the paper which was mentioned above and which you can find here.\n",
    "\n",
    "These spatial data contain 20,640 observations on housing prices with 9 economic variables:\n",
    "\n",
    "__Longitude__ refers to the angular distance of a geographic place north or south of the earth’s equator for each block group;\n",
    "\n",
    "__Latitude__ refers to the angular distance of a geographic place east or west of the earth’s equator for each block group;\n",
    "\n",
    "__Housing median__ age is the median age of the people that belong to a block group. Note that the median is the value that lies at the midpoint of a frequency distribution of observed values;\n",
    "\n",
    "__Total rooms__ is the total number of rooms in the houses per block group;\n",
    "\n",
    "__Total bedrooms__ is the total number of bedrooms in the houses per block group;\n",
    "\n",
    "__Population__ is the number of inhabitants of a block group;\n",
    "\n",
    "__Households__ refers to units of houses and their occupants per block group;\n",
    "\n",
    "__Median income__ is used to register the median income of people that belong to a block group;\n",
    "\n",
    "__Median house value__ is the dependent variable and refers to the median house value per block group.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Running Spark and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load in the data\n",
    "rdd = sc.textFile('C:/Users/NicatZeynalov/Downloads/Documents/cal_housing.data')\n",
    "\n",
    "# Load in the header\n",
    "header = sc.textFile('C:/Users/NicatZeynalov/Downloads/Documents/cal_housing.domain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude: continuous.',\n",
       " 'latitude: continuous.',\n",
       " 'housingMedianAge: continuous. ',\n",
       " 'totalRooms: continuous. ',\n",
       " 'totalBedrooms: continuous. ',\n",
       " 'population: continuous. ',\n",
       " 'households: continuous. ',\n",
       " 'medianIncome: continuous. ',\n",
       " 'medianHouseValue: continuous. ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to be careful when using collect()! Running this line of code can possibly cause the driver to run out of memory. That’s why the following approach with the take() method is a safer approach if you want to just print a few elements of the RDD. In general, it’s a good principle to limit our result set whenever possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-122.230000,37.880000,41.000000,880.000000,129.000000,322.000000,126.000000,8.325200,452600.000000',\n",
       " '-122.220000,37.860000,21.000000,7099.000000,1106.000000,2401.000000,1138.000000,8.301400,358500.000000',\n",
       " '-122.240000,37.850000,52.000000,1467.000000,190.000000,496.000000,177.000000,7.257400,352100.000000',\n",
       " '-122.250000,37.850000,52.000000,1274.000000,235.000000,558.000000,219.000000,5.643100,341300.000000',\n",
       " '-122.250000,37.850000,52.000000,1627.000000,280.000000,565.000000,259.000000,3.846200,342200.000000',\n",
       " '-122.250000,37.850000,52.000000,919.000000,213.000000,413.000000,193.000000,4.036800,269700.000000',\n",
       " '-122.250000,37.840000,52.000000,2535.000000,489.000000,1094.000000,514.000000,3.659100,299200.000000',\n",
       " '-122.250000,37.840000,52.000000,3104.000000,687.000000,1157.000000,647.000000,3.120000,241400.000000',\n",
       " '-122.260000,37.840000,42.000000,2555.000000,665.000000,1206.000000,595.000000,2.080400,226700.000000',\n",
       " '-122.250000,37.840000,52.000000,3549.000000,707.000000,1551.000000,714.000000,3.691200,261100.000000']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['-122.230000',\n",
       "  '37.880000',\n",
       "  '41.000000',\n",
       "  '880.000000',\n",
       "  '129.000000',\n",
       "  '322.000000',\n",
       "  '126.000000',\n",
       "  '8.325200',\n",
       "  '452600.000000'],\n",
       " ['-122.220000',\n",
       "  '37.860000',\n",
       "  '21.000000',\n",
       "  '7099.000000',\n",
       "  '1106.000000',\n",
       "  '2401.000000',\n",
       "  '1138.000000',\n",
       "  '8.301400',\n",
       "  '358500.000000']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split lines on commas\n",
    "rdd = rdd.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Inspect the first 2 lines \n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the RDD to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules \n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Map the RDD to a DF\n",
    "df = rdd.map(lambda line: Row(longitude=line[0], \n",
    "                              latitude=line[1], \n",
    "                              housingMedianAge=line[2],\n",
    "                              totalRooms=line[3],\n",
    "                              totalBedRooms=line[4],\n",
    "                              population=line[5], \n",
    "                              households=line[6],\n",
    "                              medianIncome=line[7],\n",
    "                              medianHouseValue=line[8])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+---------+-----------+----------------+------------+-----------+-------------+-----------+\n",
      "| households|housingMedianAge| latitude|  longitude|medianHouseValue|medianIncome| population|totalBedRooms| totalRooms|\n",
      "+-----------+----------------+---------+-----------+----------------+------------+-----------+-------------+-----------+\n",
      "| 126.000000|       41.000000|37.880000|-122.230000|   452600.000000|    8.325200| 322.000000|   129.000000| 880.000000|\n",
      "|1138.000000|       21.000000|37.860000|-122.220000|   358500.000000|    8.301400|2401.000000|  1106.000000|7099.000000|\n",
      "| 177.000000|       52.000000|37.850000|-122.240000|   352100.000000|    7.257400| 496.000000|   190.000000|1467.000000|\n",
      "| 219.000000|       52.000000|37.850000|-122.250000|   341300.000000|    5.643100| 558.000000|   235.000000|1274.000000|\n",
      "| 259.000000|       52.000000|37.850000|-122.250000|   342200.000000|    3.846200| 565.000000|   280.000000|1627.000000|\n",
      "| 193.000000|       52.000000|37.850000|-122.250000|   269700.000000|    4.036800| 413.000000|   213.000000| 919.000000|\n",
      "| 514.000000|       52.000000|37.840000|-122.250000|   299200.000000|    3.659100|1094.000000|   489.000000|2535.000000|\n",
      "| 647.000000|       52.000000|37.840000|-122.250000|   241400.000000|    3.120000|1157.000000|   687.000000|3104.000000|\n",
      "| 595.000000|       42.000000|37.840000|-122.260000|   226700.000000|    2.080400|1206.000000|   665.000000|2555.000000|\n",
      "| 714.000000|       52.000000|37.840000|-122.250000|   261100.000000|    3.691200|1551.000000|   707.000000|3549.000000|\n",
      "| 402.000000|       52.000000|37.850000|-122.260000|   281500.000000|    3.203100| 910.000000|   434.000000|2202.000000|\n",
      "| 734.000000|       52.000000|37.850000|-122.260000|   241800.000000|    3.270500|1504.000000|   752.000000|3503.000000|\n",
      "| 468.000000|       52.000000|37.850000|-122.260000|   213500.000000|    3.075000|1098.000000|   474.000000|2491.000000|\n",
      "| 174.000000|       52.000000|37.840000|-122.260000|   191300.000000|    2.673600| 345.000000|   191.000000| 696.000000|\n",
      "| 620.000000|       52.000000|37.850000|-122.260000|   159200.000000|    1.916700|1212.000000|   626.000000|2643.000000|\n",
      "| 264.000000|       50.000000|37.850000|-122.260000|   140000.000000|    2.125000| 697.000000|   283.000000|1120.000000|\n",
      "| 331.000000|       52.000000|37.850000|-122.270000|   152500.000000|    2.775000| 793.000000|   347.000000|1966.000000|\n",
      "| 303.000000|       52.000000|37.850000|-122.270000|   155500.000000|    2.120200| 648.000000|   293.000000|1228.000000|\n",
      "| 419.000000|       50.000000|37.840000|-122.260000|   158700.000000|    1.991100| 990.000000|   455.000000|2239.000000|\n",
      "| 275.000000|       52.000000|37.840000|-122.270000|   162900.000000|    2.603300| 690.000000|   298.000000|1503.000000|\n",
      "+-----------+----------------+---------+-----------+----------------+------------+-----------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#returning the columns of dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- households: string (nullable = true)\n",
      " |-- housingMedianAge: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- medianHouseValue: string (nullable = true)\n",
      " |-- medianIncome: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      " |-- totalBedRooms: string (nullable = true)\n",
      " |-- totalRooms: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#returning the schema of dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All columns are still of data type string… That’s disappointing!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "  for name in names: \n",
    "     df = df.withColumn(name, df[name].cast(newType))\n",
    "  return df \n",
    "\n",
    "# Assign all column names to `columns`\n",
    "columns = ['households', 'housingMedianAge', 'latitude', 'longitude', 'medianHouseValue', 'medianIncome', 'population', 'totalBedRooms', 'totalRooms']\n",
    "\n",
    "# Conver the `df` columns to `FloatType()`\n",
    "df = convertColumn(df, columns, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n",
      "|households|housingMedianAge|latitude|longitude|medianHouseValue|medianIncome|population|totalBedRooms|totalRooms|\n",
      "+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n",
      "|     126.0|            41.0|   37.88|  -122.23|        452600.0|      8.3252|     322.0|        129.0|     880.0|\n",
      "|    1138.0|            21.0|   37.86|  -122.22|        358500.0|      8.3014|    2401.0|       1106.0|    7099.0|\n",
      "|     177.0|            52.0|   37.85|  -122.24|        352100.0|      7.2574|     496.0|        190.0|    1467.0|\n",
      "|     219.0|            52.0|   37.85|  -122.25|        341300.0|      5.6431|     558.0|        235.0|    1274.0|\n",
      "|     259.0|            52.0|   37.85|  -122.25|        342200.0|      3.8462|     565.0|        280.0|    1627.0|\n",
      "|     193.0|            52.0|   37.85|  -122.25|        269700.0|      4.0368|     413.0|        213.0|     919.0|\n",
      "|     514.0|            52.0|   37.84|  -122.25|        299200.0|      3.6591|    1094.0|        489.0|    2535.0|\n",
      "|     647.0|            52.0|   37.84|  -122.25|        241400.0|        3.12|    1157.0|        687.0|    3104.0|\n",
      "|     595.0|            42.0|   37.84|  -122.26|        226700.0|      2.0804|    1206.0|        665.0|    2555.0|\n",
      "|     714.0|            52.0|   37.84|  -122.25|        261100.0|      3.6912|    1551.0|        707.0|    3549.0|\n",
      "|     402.0|            52.0|   37.85|  -122.26|        281500.0|      3.2031|     910.0|        434.0|    2202.0|\n",
      "|     734.0|            52.0|   37.85|  -122.26|        241800.0|      3.2705|    1504.0|        752.0|    3503.0|\n",
      "|     468.0|            52.0|   37.85|  -122.26|        213500.0|       3.075|    1098.0|        474.0|    2491.0|\n",
      "|     174.0|            52.0|   37.84|  -122.26|        191300.0|      2.6736|     345.0|        191.0|     696.0|\n",
      "|     620.0|            52.0|   37.85|  -122.26|        159200.0|      1.9167|    1212.0|        626.0|    2643.0|\n",
      "|     264.0|            50.0|   37.85|  -122.26|        140000.0|       2.125|     697.0|        283.0|    1120.0|\n",
      "|     331.0|            52.0|   37.85|  -122.27|        152500.0|       2.775|     793.0|        347.0|    1966.0|\n",
      "|     303.0|            52.0|   37.85|  -122.27|        155500.0|      2.1202|     648.0|        293.0|    1228.0|\n",
      "|     419.0|            50.0|   37.84|  -122.26|        158700.0|      1.9911|     990.0|        455.0|    2239.0|\n",
      "|     275.0|            52.0|   37.84|  -122.27|        162900.0|      2.6033|     690.0|        298.0|    1503.0|\n",
      "+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start small and just select two columns from df of which you only want to see 10 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|households|medianIncome|\n",
      "+----------+------------+\n",
      "|     126.0|      8.3252|\n",
      "|    1138.0|      8.3014|\n",
      "|     177.0|      7.2574|\n",
      "|     219.0|      5.6431|\n",
      "|     259.0|      3.8462|\n",
      "|     193.0|      4.0368|\n",
      "|     514.0|      3.6591|\n",
      "|     647.0|        3.12|\n",
      "|     595.0|      2.0804|\n",
      "|     714.0|      3.6912|\n",
      "+----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('households', 'medianIncome').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|summary|        households|  housingMedianAge|          latitude|          longitude|  medianHouseValue|      medianIncome|        population|    totalBedRooms|        totalRooms|\n",
      "+-------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|  count|             20640|             20640|             20640|              20640|             20640|             20640|             20640|            20640|             20640|\n",
      "|   mean| 499.5396802325581|28.639486434108527| 35.63186143109965|-119.56970444871473|206855.81690891474|3.8706710030346416|1425.4767441860465|537.8980135658915|2635.7630813953488|\n",
      "| stddev|382.32975283161136|12.585557612111613|2.1359523806029554| 2.0035317429328914|115395.61587441381|1.8998217183639672|1132.4621217653385|421.2479059431315|2181.6152515827994|\n",
      "|    min|               1.0|               1.0|             32.54|            -124.35|           14999.0|            0.4999|               3.0|              1.0|               2.0|\n",
      "|    max|            6082.0|              52.0|             41.95|            -114.31|          500001.0|           15.0001|           35682.0|           6445.0|           39320.0|\n",
      "+-------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Adjust the values of `medianHouseValue`\n",
    "df = df.withColumn(\"medianHouseValue\", col(\"medianHouseValue\")/100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n",
      "|households|housingMedianAge|latitude|longitude|medianHouseValue|medianIncome|population|totalBedRooms|totalRooms|\n",
      "+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n",
      "|     126.0|            41.0|   37.88|  -122.23|           4.526|      8.3252|     322.0|        129.0|     880.0|\n",
      "|    1138.0|            21.0|   37.86|  -122.22|           3.585|      8.3014|    2401.0|       1106.0|    7099.0|\n",
      "+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Future Engineering\n",
    "Now that I have adjusted the values in medianHouseValue, I can also add the additional variables that I read about above. I am going to add the following columns to the data set:\n",
    "\n",
    "__Rooms per household which refers to the number of rooms in households per block group;__\n",
    "\n",
    "__Population per household, which basically gives me an indication of how many people live in households per block group;__\n",
    "\n",
    "__Bedrooms per room which will give me an idea about how many rooms are bedrooms per block group;__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(households=126.0, housingMedianAge=41.0, latitude=37.880001068115234, longitude=-122.2300033569336, medianHouseValue=4.526, medianIncome=8.325200080871582, population=322.0, totalBedRooms=129.0, totalRooms=880.0, roomsPerHousehold=6.984126984126984, personPerHousehold=2.5555555555555554, bedroomsPerRoom=0.14659090909090908)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all from `sql.functions` if you haven't yet\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Divide `totalRooms` by `households`\n",
    "roomsPerHousehold = df.select(col('totalRooms')/col('households'))\n",
    "\n",
    "#Divide 'population' by 'households'\n",
    "personPerHousehold = df.select(col('population')/col('households'))\n",
    "\n",
    "# Divide `totalBedRooms` by `totalRooms`\n",
    "bedroomsPerRoom = df.select(col('totalBedRooms')/col('totalRooms'))\n",
    "\n",
    "# Add the new columns to `df`\n",
    "df = df.withColumn('roomsPerHousehold', col('totalRooms')/col('households')).withColumn('personPerHousehold',col('population')/col('households')).withColumn('bedroomsPerRoom',col('totalBedRooms')/col('totalRooms'))\n",
    "\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-order and select columns\n",
    "df = df.select(\"medianHouseValue\", \n",
    "              \"totalBedRooms\", \n",
    "              \"population\", \n",
    "              \"households\", \n",
    "              \"medianIncome\", \n",
    "              \"roomsPerHousehold\", \n",
    "              \"personPerHousehold\", \n",
    "              \"bedroomsPerRoom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Standardization\n",
    "Now that I have re-ordered the data, I am ready to normalize the data. Or almost, at least. There is just one more step that I need to go through: separating the features from the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `DenseVector`\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Define the `input_data` \n",
    "input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# Replace `df` with the new DataFrame\n",
    "df = spark.createDataFrame(input_data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|4.526|[129.0,322.0,126....|\n",
      "|3.585|[1106.0,2401.0,11...|\n",
      "|3.521|[190.0,496.0,177....|\n",
      "|3.413|[235.0,558.0,219....|\n",
      "|3.422|[280.0,565.0,259....|\n",
      "|2.697|[213.0,413.0,193....|\n",
      "|2.992|[489.0,1094.0,514...|\n",
      "|2.414|[687.0,1157.0,647...|\n",
      "|2.267|[665.0,1206.0,595...|\n",
      "|2.611|[707.0,1551.0,714...|\n",
      "|2.815|[434.0,910.0,402....|\n",
      "|2.418|[752.0,1504.0,734...|\n",
      "|2.135|[474.0,1098.0,468...|\n",
      "|1.913|[191.0,345.0,174....|\n",
      "|1.592|[626.0,1212.0,620...|\n",
      "|  1.4|[283.0,697.0,264....|\n",
      "|1.525|[347.0,793.0,331....|\n",
      "|1.555|[293.0,648.0,303....|\n",
      "|1.587|[455.0,990.0,419....|\n",
      "|1.629|[298.0,690.0,275....|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __Next, I can finally scale the data. I can use Spark ML to do this: this library will make machine learning on big data scalable and easy.__ The input columns are the features, and the output column with the rescaled that will be included in the scaled_df will be named \"features_scaled\":\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=4.526, features=DenseVector([129.0, 322.0, 126.0, 8.3252, 6.9841, 2.5556, 0.1466]), features_scaled=DenseVector([0.3062, 0.2843, 0.3296, 4.3821, 2.8228, 0.2461, 2.5264])),\n",
       " Row(label=3.585, features=DenseVector([1106.0, 2401.0, 1138.0, 8.3014, 6.2381, 2.1098, 0.1558]), features_scaled=DenseVector([2.6255, 2.1202, 2.9765, 4.3696, 2.5213, 0.2031, 2.6851]))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import `StandardScaler` \n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Initialize the `standardScaler`\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "# Fit the DataFrame to the scaler\n",
    "scaler = standardScaler.fit(df)\n",
    "\n",
    "# Transform the data in `df` with the scaler\n",
    "scaled_df = scaler.transform(df)\n",
    "\n",
    "# Inspect the result\n",
    "scaled_df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building A Machine Learning Model With Spark ML\n",
    "With all the preprocessing done, it’s finally time to start building my Linear Regression model! Just like always, I first need to split the data into training and test sets. Luckily, this is no issue with the randomSplit() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `LinearRegression`\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LinearRegression(labelCol=\"label\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note that__ the argument elasticNetParam corresponds to α or the vertical intercept and that the regParam or the regularization paramater corresponds to λ. Go here for more information. Lastly, I can then inspect the predicted and real values by simply accessing the list with square brackets []:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.5755352395152893, 0.14999),\n",
       " (1.7341233026776406, 0.225),\n",
       " (1.531875281086717, 0.388),\n",
       " (1.3628617899920583, 0.394),\n",
       " (1.2862982565956895, 0.396)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "predicted = linearModel.transform(test_data)\n",
    "\n",
    "# Extract the predictions and the \"known\" correct labels\n",
    "predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "labels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n",
    "\n",
    "# Zip `predictions` and `labels` into a list\n",
    "predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "# Print out first 5 instances of `predictionAndLabel` \n",
    "predictionAndLabel[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating the Model\n",
    "Looking at predicted values is one thing, but another and better thing is looking at some metrics to get a better idea of how good my model actually is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9963441266477807"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coefficients for the model\n",
    "linearModel.coefficients\n",
    "\n",
    "# Intercept for the model\n",
    "linearModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
